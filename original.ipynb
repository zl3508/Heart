{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install category_encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data analysis related library\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# some visualization related library\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "from matplotlib.colors import LogNorm\n",
    "import seaborn as sns\n",
    "\n",
    "# category_encoders and warning related library\n",
    "from warnings import simplefilter\n",
    "simplefilter(action='ignore')\n",
    "import category_encoders\n",
    "\n",
    "# sklearn related ML library\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, RocCurveDisplay\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.utils import resample\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "# other useful libray\n",
    "import missingno as msno\n",
    "from mpl_toolkits.axes_grid1.inset_locator import mark_inset\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "from pylab import subplots_adjust\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data analysis related library\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# some visualization related library\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "from matplotlib.colors import LogNorm\n",
    "import seaborn as sns\n",
    "\n",
    "# category_encoders and warning related library\n",
    "from warnings import simplefilter\n",
    "simplefilter(action='ignore')\n",
    "import category_encoders\n",
    "\n",
    "# sklearn related ML library\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, RocCurveDisplay\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.utils import resample\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "# other useful libray\n",
    "import missingno as msno\n",
    "from mpl_toolkits.axes_grid1.inset_locator import mark_inset\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "from pylab import subplots_adjust\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "health_data = pd.read_csv(\"framingham.csv\")\n",
    "health_data.info()\n",
    "health_data.shape\n",
    "health_data.head()\n",
    "health_data.describe()\n",
    "health_data.isna().sum().sum()\n",
    "health_data.isnull().sum().sort_values(ascending=False)\n",
    "(health_data.isnull().sum() / health_data.isnull().count()).sort_values(ascending=False)\n",
    "msno.dendrogram(health_data)\n",
    "msno.heatmap(health_data)\n",
    "msno.matrix(health_data, color=(0.6, 0.2, 0.8)) \n",
    "msno.matrix(health_data,color=(47/255,127/255,255/255))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "missing_counts = health_data.isnull().sum()\n",
    "\n",
    "num_columns = len(missing_counts)\n",
    "half = num_columns // 2\n",
    "cols_part1 = missing_counts[:half]\n",
    "cols_part2 = missing_counts[half:]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))  \n",
    "\n",
    "axes[0].barh(cols_part1.index, cols_part1.values, color=(255/255, 151/255, 0/255))\n",
    "#axes[0].set_title(\"Missing Values\")\n",
    "axes[0].set_xlabel(\"Count\")\n",
    "\n",
    "axes[1].barh(cols_part2.index, cols_part2.values, color=(255/255, 151/255, 0/255))\n",
    "#axes[1].set_title(\"Missing Values\")\n",
    "axes[1].set_xlabel(\"Count\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.bar(health_data.sample(1000), color=(255/255,151/225,0/255))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "health_data.TenYearCHD.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features = health_data.columns[health_data.dtypes != object]\n",
    "list(numerical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = len(health_data.columns)  \n",
    "num_rows = 2  \n",
    "num_cols = (num_features + num_rows - 1) // num_rows \n",
    "\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(num_cols * 4, num_rows * 4))\n",
    "\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(health_data.columns):\n",
    "    sns.histplot(x=col, data=health_data, ax=axes[i], color=(159/255, 57/255, 222/255))\n",
    "\n",
    "    axes[i].set_xlabel(col, fontsize=16) \n",
    "    axes[i].set_ylabel(\"Count\", fontsize=16)  \n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "health_data_filled_with_mean = pd.DataFrame()\n",
    "for col in health_data.columns:\n",
    "    if health_data[col].dtype == 'float64':\n",
    "        health_data_filled_with_mean[col] = health_data[col].fillna(health_data[col].mean())\n",
    "    elif set(health_data[col].dropna().unique()).issubset({0, 1}):\n",
    "        health_data_filled_with_mean[col] = health_data[col].fillna(health_data[col].median())\n",
    "    else:\n",
    "        health_data_filled_with_mean[col] = health_data[col]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = len(health_data_filled_with_mean.columns)  \n",
    "num_rows = 2  \n",
    "num_cols = (num_features + num_rows - 1) // num_rows  \n",
    "\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(num_cols * 4, num_rows * 4))\n",
    "\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(health_data_filled_with_mean.columns):\n",
    "    sns.histplot(x=col, data=health_data_filled_with_mean, ax=axes[i], color=(159/255, 57/255, 222/255))\n",
    "\n",
    "    axes[i].set_xlabel(col, fontsize=16)\n",
    "    axes[i].set_ylabel(\"Count\", fontsize=16)\n",
    "\n",
    "    axes[i].tick_params(axis='x', labelsize=14)\n",
    "    axes[i].tick_params(axis='y', labelsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "health_data_filled_with_mean.isnull().sum().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "health_data_filled_with_mean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "health_data_filled_with_mean['BPMeds'] = health_data_filled_with_mean['BPMeds'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "health_data_filled_with_mean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.rcParams.update({\n",
    "    'font.size': 18,            \n",
    "    'font.weight': 'normal',    \n",
    "    'axes.labelweight': 'normal',\n",
    "    'axes.titleweight': 'normal'\n",
    "})\n",
    "\n",
    "fig = plt.figure(figsize=(35, 40))\n",
    "fig.set_facecolor('white')  \n",
    "\n",
    "index = 1\n",
    "target_col = 'TenYearCHD'\n",
    "\n",
    "box_color = (0/255, 100/255, 200/255)  \n",
    "\n",
    "for k in health_data_filled_with_mean:\n",
    "    if health_data_filled_with_mean[k].dtype == 'float64':\n",
    "        ax = plt.subplot(len(numerical_features), 2, index)\n",
    "\n",
    "        sns.boxplot(\n",
    "            x=health_data_filled_with_mean[target_col].astype(str),  \n",
    "            y=health_data_filled_with_mean[k],\n",
    "            color=box_color  \n",
    "        )\n",
    "\n",
    "        plt.title(f'Box Plot of {k} by {target_col}', fontsize=12, fontweight='normal')\n",
    "        plt.xlabel('TenYearCHD', fontsize=12, fontweight='normal')\n",
    "        plt.ylabel(k, fontsize=12, fontweight='normal')\n",
    "\n",
    "        ax.yaxis.grid(True, linestyle='--', alpha=0.7)\n",
    "        for spine in ax.spines.values():\n",
    "            spine.set_edgecolor('darkblue')  \n",
    "            spine.set_linewidth(2)  \n",
    "            spine.set_linestyle('-')  \n",
    "\n",
    "        ax.spines['top'].set_visible(False)  \n",
    "        ax.spines['right'].set_visible(False)  \n",
    "        ax.spines['left'].set_color('black')  \n",
    "        ax.spines['bottom'].set_color('black')  \n",
    "\n",
    "        index += 1\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "fig = plt.figure(figsize=(35, 40))\n",
    "fig.set_facecolor((235/255, 235/255, 235/255))\n",
    "\n",
    "index = 1\n",
    "target_col = 'TenYearCHD'\n",
    "\n",
    "for k in health_data_filled_with_mean:\n",
    "    if health_data_filled_with_mean[k].dtype == 'float64':\n",
    "        plt.subplot(len(numerical_features), 2, index)\n",
    "        sns.boxplot(\n",
    "            x=health_data_filled_with_mean[target_col].astype(str),  \n",
    "            y=health_data_filled_with_mean[k],\n",
    "            palette={'0': (0/255, 219/255, 197/255), '1': (255/255, 99/255, 132/255)}   \n",
    "        )\n",
    "        plt.title(f'Box Plot of {k} by {target_col}')\n",
    "        plt.xlabel('TenYearCHD')\n",
    "        plt.ylabel(k)\n",
    "        index += 1\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "fig = plt.figure(figsize=(35, 40))\n",
    "fig.set_facecolor((235/255, 235/255, 235/255))\n",
    "\n",
    "index = 1\n",
    "target_col = 'TenYearCHD'\n",
    "\n",
    "for k in health_data_filled_with_mean:\n",
    "    if health_data_filled_with_mean[k].dtype == 'float64':\n",
    "        plt.subplot(len(numerical_features), 2, index)\n",
    "        sns.boxplot(\n",
    "            x=health_data_filled_with_mean[target_col].astype(str), \n",
    "            y=health_data_filled_with_mean[k],\n",
    "            palette={'0': (0/255, 219/255, 197/255), '1': (255/255, 99/255, 132/255)}\n",
    "        )\n",
    "        plt.title(f'Box Plot of {k} by {target_col}', fontsize=20)  \n",
    "        plt.xlabel('TenYearCHD', fontsize=18)  \n",
    "        plt.ylabel(k, fontsize=18)  \n",
    "        plt.xticks(fontsize=16)  \n",
    "        plt.yticks(fontsize=16) \n",
    "        index += 1\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_0 = health_data_filled_with_mean[health_data_filled_with_mean['TenYearCHD'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(35,20))\n",
    "fig.set_facecolor((235/255,235/255,235/255))\n",
    "index = 1\n",
    "for k in group_0:\n",
    "    if group_0[k].dtype=='float64':\n",
    "        plt.subplot(4, 6, index)\n",
    "        sns.boxplot(group_0[k], color=(0/255,219/255,197/255))\n",
    "        index += 1\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in group_0.columns:\n",
    "    if col != 'TenYearCHD' and group_0[col].dtype == 'float64':\n",
    "        Q1 = group_0[col].quantile(0.25)\n",
    "        Q3 = group_0[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "        group_0[col] = np.where(group_0[col] < lower_bound, lower_bound, group_0[col])\n",
    "        group_0[col] = np.where(group_0[col] > upper_bound, upper_bound, group_0[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(35,20))\n",
    "fig.set_facecolor((235/255,235/255,235/255))\n",
    "index = 1\n",
    "for k in group_0:\n",
    "    if group_0[k].dtype=='float64':\n",
    "        plt.subplot(4, 6, index)\n",
    "        sns.boxplot(group_0[k], color=(0/255,219/255,197/255))\n",
    "        index += 1\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_1 = health_data_filled_with_mean[health_data_filled_with_mean['TenYearCHD'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(35,20))\n",
    "fig.set_facecolor((235/255,235/255,235/255))\n",
    "index = 1\n",
    "for k in group_1:\n",
    "    if group_1[k].dtype=='float64':\n",
    "        plt.subplot(4, 6, index)\n",
    "        sns.boxplot(group_1[k], color=(0/255,219/255,197/255))\n",
    "        index += 1\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in group_1.columns:\n",
    "    if col != 'TenYearCHD' and group_1[col].dtype == 'float64':\n",
    "        Q1 = group_1[col].quantile(0.25)\n",
    "        Q3 = group_1[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "        group_1[col] = np.where(group_1[col] < lower_bound, lower_bound, group_1[col])\n",
    "        group_1[col] = np.where(group_1[col] > upper_bound, upper_bound, group_1[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(35,20))\n",
    "fig.set_facecolor((235/255,235/255,235/255))\n",
    "index = 1\n",
    "for k in group_1:\n",
    "    if group_1[k].dtype=='float64':\n",
    "        plt.subplot(4, 6, index)\n",
    "        sns.boxplot(group_1[k], color=(0/255,219/255,197/255))\n",
    "        index += 1\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "health_data_filled_with_mean = pd.concat([group_0, group_1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "plt.rcParams.update({\n",
    "    'font.size': 18,            \n",
    "    'font.weight': 'normal',    \n",
    "    'axes.labelweight': 'normal',\n",
    "    'axes.titleweight': 'normal'\n",
    "})\n",
    "\n",
    "fig = plt.figure(figsize=(35, 40))\n",
    "fig.set_facecolor('white') \n",
    "\n",
    "index = 1\n",
    "target_col = 'TenYearCHD'\n",
    "\n",
    "box_color = (0/255, 100/255, 200/255)  \n",
    "\n",
    "for k in health_data_filled_with_mean:\n",
    "    if health_data_filled_with_mean[k].dtype == 'float64':\n",
    "        ax = plt.subplot(len(numerical_features), 2, index)\n",
    "\n",
    "        sns.boxplot(\n",
    "            x=health_data_filled_with_mean[target_col].astype(str),  \n",
    "            y=health_data_filled_with_mean[k],\n",
    "            color=box_color  \n",
    "        )\n",
    "\n",
    "        plt.title(f'Box Plot of {k} by {target_col}', fontsize=12, fontweight='normal')\n",
    "        plt.xlabel('TenYearCHD', fontsize=12, fontweight='normal')\n",
    "        plt.ylabel(k, fontsize=12, fontweight='normal')\n",
    "\n",
    "        ax.yaxis.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "        for spine in ax.spines.values():\n",
    "            spine.set_edgecolor('darkblue')  \n",
    "            spine.set_linewidth(2)  \n",
    "            spine.set_linestyle('-')  \n",
    "\n",
    "        ax.spines['top'].set_visible(False)  \n",
    "        ax.spines['right'].set_visible(False)  \n",
    "        ax.spines['left'].set_color('black')  \n",
    "        ax.spines['bottom'].set_color('black')  \n",
    "\n",
    "        index += 1\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "fig = plt.figure(figsize=(35, 40))\n",
    "fig.set_facecolor((235/255, 235/255, 235/255))\n",
    "\n",
    "index = 1\n",
    "target_col = 'TenYearCHD'\n",
    "\n",
    "for k in health_data_filled_with_mean:\n",
    "    if health_data_filled_with_mean[k].dtype == 'float64':\n",
    "        plt.subplot(len(numerical_features), 2, index)\n",
    "        sns.boxplot(\n",
    "            x=health_data_filled_with_mean[target_col].astype(str),  \n",
    "            y=health_data_filled_with_mean[k],\n",
    "            palette={'0': (0/255, 219/255, 197/255), '1': (255/255, 99/255, 132/255)}\n",
    "        )\n",
    "        plt.title(f'Box Plot of {k} by {target_col}')\n",
    "        plt.xlabel('TenYearCHD')\n",
    "        plt.ylabel(k)\n",
    "        index += 1\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "fig = plt.figure(figsize=(35, 40))\n",
    "fig.set_facecolor((235/255, 235/255, 235/255))\n",
    "index = 1\n",
    "target_col = 'TenYearCHD'\n",
    "for k in health_data_filled_with_mean:\n",
    "    if health_data_filled_with_mean[k].dtype == 'float64':\n",
    "        plt.subplot(len(numerical_features), 2, index)\n",
    "        sns.boxplot(\n",
    "            x=health_data_filled_with_mean[target_col].astype(str),\n",
    "            y=health_data_filled_with_mean[k],\n",
    "            palette={'0': (0/255, 219/255, 197/255), '1': (255/255, 99/255, 132/255)}\n",
    "        )\n",
    "        plt.title(f'Box Plot of {k} by {target_col}', fontsize=20)  \n",
    "        plt.xlabel('TenYearCHD', fontsize=18)\n",
    "        plt.ylabel(k, fontsize=18)  \n",
    "        plt.xticks(fontsize=16)  \n",
    "        plt.yticks(fontsize=16)  \n",
    "        index += 1\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure(figsize=(35, 20))\n",
    "fig.set_facecolor((235/255, 235/255, 235/255))\n",
    "index = 1\n",
    "\n",
    "for k in group_0:\n",
    "    if group_0[k].dtype == 'float64':\n",
    "        plt.subplot(4, 6, index)\n",
    "        sns.boxplot(group_0[k], color=(0/255, 219/255, 197/255))\n",
    "        plt.title(f'Box Plot of {k} (TenYearCHD=0)', fontsize=18)\n",
    "        plt.xlabel(k, fontsize=16)\n",
    "        plt.xticks(fontsize=14)\n",
    "        plt.yticks(fontsize=14)\n",
    "        index += 1\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure(figsize=(35, 20))\n",
    "fig.set_facecolor((235/255, 235/255, 235/255))\n",
    "index = 1\n",
    "\n",
    "for k in group_1:\n",
    "    if group_1[k].dtype == 'float64':\n",
    "        plt.subplot(4, 6, index)\n",
    "        sns.boxplot(group_1[k], color=(255/255, 99/255, 132/255))\n",
    "        plt.title(f'Box Plot of {k} (TenYearCHD=1)', fontsize=18)\n",
    "        plt.xlabel(k, fontsize=16)\n",
    "        plt.xticks(fontsize=14)\n",
    "        plt.yticks(fontsize=14)\n",
    "        index += 1\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(health_data_filled_with_mean)\n",
    "scaled_data = pd.DataFrame(scaled_data, columns=health_data_filled_with_mean.columns)\n",
    "scaled_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(health_data_filled_with_mean)\n",
    "scaled_data = pd.DataFrame(scaled_data, columns=health_data_filled_with_mean.columns)\n",
    "scaled_data.info()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "X = scaled_data.drop(columns=['TenYearCHD'])\n",
    "y = scaled_data['TenYearCHD']\n",
    "\n",
    "x_train, x_validation, y_train, y_validation = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "x_train_upsampled, y_train_upsampled = smote.fit_resample(x_train, y_train)\n",
    "\n",
    "from collections import Counter\n",
    "print('Original training dataset shape:', Counter(y_train))\n",
    "print('Upsampled training dataset shape:', Counter(y_train_upsampled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_grid = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [10, 20, 30, 40, 50],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2],\n",
    "    'max_features': ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "# Perform grid search for hyperparameter optimization\n",
    "grid_search = GridSearchCV(\n",
    "    DecisionTreeClassifier(random_state=42),\n",
    "    param_grid=parameter_grid,\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "grid_search.fit(x_train_upsampled, y_train_upsampled)\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "print(f\"Optimal parameters: {best_params}\")\n",
    "\n",
    "# Train the best decision tree classifier\n",
    "best_classifier = DecisionTreeClassifier(**best_params, random_state=42)\n",
    "best_classifier.fit(x_train_upsampled, y_train_upsampled)\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "y_pred = best_classifier.predict(x_validation)\n",
    "y_pred_proba = best_classifier.predict_proba(x_validation)[:, 1]\n",
    "\n",
    "# Calculate metrics\n",
    "acc = metrics.accuracy_score(y_validation, y_pred)\n",
    "precision = metrics.precision_score(y_validation, y_pred)\n",
    "recall = metrics.recall_score(y_validation, y_pred)\n",
    "f1 = metrics.f1_score(y_validation, y_pred)\n",
    "auc = metrics.roc_auc_score(y_validation, y_pred_proba)\n",
    "fpr, tpr, _ = metrics.roc_curve(y_validation, y_pred_proba)\n",
    "\n",
    "# Store results in a DataFrame\n",
    "results = [{\n",
    "    'Label': '8:1:1',\n",
    "    'Accuracy': \"{:.3f}\".format(acc),\n",
    "    'Precision': \"{:.3f}\".format(precision),\n",
    "    'Recall': \"{:.3f}\".format(recall),\n",
    "    'ROC AUC': \"{:.3f}\".format(auc),\n",
    "    'F1 Score': \"{:.3f}\".format(f1)\n",
    "}]\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "print(df[['Label', 'Accuracy', 'Precision', 'Recall', 'ROC AUC', 'F1 Score']])\n",
    "\n",
    "# Plot the ROC Curve\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "ax.plot(fpr, tpr, label=f\"ROC AUC = {auc:.3f}\", color='b')\n",
    "ax.plot([0, 1], [0, 1], 'k--', lw=2, alpha=0.7)\n",
    "ax.legend(loc=\"lower right\")\n",
    "ax.set_xlabel('False Positive Rate')\n",
    "ax.set_ylabel('True Positive Rate')\n",
    "ax.set_title(\"ROC Curve for Optimized Decision Tree\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],  # Regularization strength\n",
    "    'solver': ['lbfgs', 'liblinear'],  # Optimization solvers\n",
    "    'max_iter': [1000]  # Maximum number of iterations\n",
    "}\n",
    "\n",
    "# Perform grid search for hyperparameter optimization\n",
    "grid_search = GridSearchCV(\n",
    "    LogisticRegression(random_state=42),\n",
    "    param_grid=parameter_grid,\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "grid_search.fit(x_train_upsampled, y_train_upsampled)\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "print(f\"Optimal parameters: {best_params}\")\n",
    "\n",
    "# Train the best Logistic Regression model\n",
    "best_classifier = LogisticRegression(**best_params, random_state=42)\n",
    "best_classifier.fit(x_train_upsampled, y_train_upsampled)\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "y_pred = best_classifier.predict(x_validation)\n",
    "y_pred_proba = best_classifier.predict_proba(x_validation)[:, 1]\n",
    "\n",
    "# Calculate metrics\n",
    "acc = metrics.accuracy_score(y_validation, y_pred)\n",
    "precision = metrics.precision_score(y_validation, y_pred)\n",
    "recall = metrics.recall_score(y_validation, y_pred)\n",
    "f1 = metrics.f1_score(y_validation, y_pred)\n",
    "auc = metrics.roc_auc_score(y_validation, y_pred_proba)\n",
    "fpr, tpr, _ = metrics.roc_curve(y_validation, y_pred_proba)\n",
    "\n",
    "# Store results in a DataFrame\n",
    "results = [{\n",
    "    'Label': '8:1:1',\n",
    "    'Accuracy': \"{:.3f}\".format(acc),\n",
    "    'Precision': \"{:.3f}\".format(precision),\n",
    "    'Recall': \"{:.3f}\".format(recall),\n",
    "    'ROC AUC': \"{:.3f}\".format(auc),\n",
    "    'F1 Score': \"{:.3f}\".format(f1)\n",
    "}]\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "print(df[['Label', 'Accuracy', 'Precision', 'Recall', 'ROC AUC', 'F1 Score']])\n",
    "\n",
    "# Plot the ROC Curve\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "ax.plot(fpr, tpr, label=f\"ROC AUC = {auc:.3f}\", color='b')\n",
    "ax.plot([0, 1], [0, 1], 'k--', lw=2, alpha=0.7)\n",
    "ax.legend(loc=\"lower right\")\n",
    "ax.set_xlabel('False Positive Rate')\n",
    "ax.set_ylabel('True Positive Rate')\n",
    "ax.set_title(\"ROC Curve for Optimized Logistic Regression\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_grid = {\n",
    "    'alpha': [0.1, 0.5, 1, 2, 5, 10],\n",
    "    'binarize': [0, 0.5, 1, 2]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    BernoulliNB(),\n",
    "    param_grid=parameter_grid,\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    scoring='accuracy'\n",
    ")\n",
    "\n",
    "grid_search.fit(x_train_upsampled, y_train_upsampled)\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "print(f\"Optimal parameters: {best_params}\")\n",
    "\n",
    "best_nb = BernoulliNB(**best_params)\n",
    "best_nb.fit(x_train_upsampled, y_train_upsampled)\n",
    "\n",
    "y_pred = best_nb.predict(x_validation)\n",
    "y_pred_proba = best_nb.predict_proba(x_validation)[:, 1]\n",
    "\n",
    "acc = metrics.accuracy_score(y_validation, y_pred)\n",
    "precision = metrics.precision_score(y_validation, y_pred)\n",
    "recall = metrics.recall_score(y_validation, y_pred)\n",
    "f1 = metrics.f1_score(y_validation, y_pred)\n",
    "auc = metrics.roc_auc_score(y_validation, y_pred_proba)\n",
    "fpr, tpr, _ = metrics.roc_curve(y_validation, y_pred_proba)\n",
    "\n",
    "results = [{\n",
    "    'Label': '8:1:1',\n",
    "    'Accuracy': \"{:.3f}\".format(acc),\n",
    "    'Precision': \"{:.3f}\".format(precision),\n",
    "    'Recall': \"{:.3f}\".format(recall),\n",
    "    'ROC AUC': \"{:.3f}\".format(auc),\n",
    "    'F1 Score': \"{:.3f}\".format(f1)\n",
    "}]\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "print(df[['Label', 'Accuracy', 'Precision', 'Recall', 'ROC AUC', 'F1 Score']])\n",
    "\n",
    "# Plot the ROC Curve\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "ax.plot(fpr, tpr, label=f\"ROC AUC = {auc:.3f}\", color='b')\n",
    "ax.plot([0, 1], [0, 1], 'k--', lw=2, alpha=0.7)\n",
    "ax.legend(loc=\"lower right\")\n",
    "ax.set_xlabel('False Positive Rate')\n",
    "ax.set_ylabel('True Positive Rate')\n",
    "ax.set_title(\"ROC Curve for Optimized BernoulliNB Model\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_grid = {\n",
    "    'n_neighbors': [80, 100, 200],\n",
    "    'p': [1, 2],  # 1: Manhattan distance, 2: Euclidean distance\n",
    "    'weights': ['uniform', 'distance']\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    KNeighborsClassifier(),\n",
    "    param_grid=parameter_grid,\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    scoring='accuracy'\n",
    ")\n",
    "grid_search.fit(x_train_upsampled, y_train_upsampled)\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "print(f\"Optimal parameters: {best_params}\")\n",
    "\n",
    "# Train the optimal classifier\n",
    "knn_optimal = KNeighborsClassifier(**best_params)\n",
    "knn_optimal.fit(x_train_upsampled, y_train_upsampled)\n",
    "\n",
    "# Predict on validation set\n",
    "y_pred = knn_optimal.predict(x_validation)\n",
    "y_pred_proba = knn_optimal.predict_proba(x_validation)[:, 1]\n",
    "\n",
    "# Calculate metrics\n",
    "acc = metrics.accuracy_score(y_validation, y_pred)\n",
    "precision = metrics.precision_score(y_validation, y_pred)\n",
    "recall = metrics.recall_score(y_validation, y_pred)\n",
    "f1 = metrics.f1_score(y_validation, y_pred)\n",
    "auc = metrics.roc_auc_score(y_validation, y_pred_proba)\n",
    "fpr, tpr, _ = metrics.roc_curve(y_validation, y_pred_proba)\n",
    "\n",
    "results = [{\n",
    "    'Label': '8:2',\n",
    "    'Accuracy': \"{:.3f}\".format(acc),\n",
    "    'Precision': \"{:.3f}\".format(precision),\n",
    "    'Recall': \"{:.3f}\".format(recall),\n",
    "    'ROC AUC': \"{:.3f}\".format(auc),\n",
    "    'F1 Score': \"{:.3f}\".format(f1)\n",
    "}]\n",
    "\n",
    "\n",
    "# Create DataFrame for metrics\n",
    "df_metrics = pd.DataFrame(results)\n",
    "print(df_metrics[['Label', 'Accuracy', 'Precision', 'Recall', 'ROC AUC', 'F1 Score']])\n",
    "\n",
    "# Plot the ROC Curve\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "ax.plot(fpr, tpr, label=f\"ROC AUC = {auc:.3f}\", color='b')\n",
    "ax.plot([0, 1], [0, 1], 'k--', lw=2, alpha=0.7)\n",
    "ax.legend(loc=\"lower right\")\n",
    "ax.set_xlabel('False Positive Rate')\n",
    "ax.set_ylabel('True Positive Rate')\n",
    "ax.set_title(\"ROC Curve for Optimized KNN Model\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter_grid = {\n",
    "#     'n_estimators': [10, 100, 1000, 1200],\n",
    "#     'learning_rate': [0.1, 0.2, 0.4, 0.8]\n",
    "# }\n",
    "\n",
    "parameter_grid = {\n",
    "    'n_estimators': [10, 100, 500],\n",
    "    'learning_rate': [0.1, 0.2, 0.4]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    GradientBoostingClassifier(random_state=42),\n",
    "    param_grid=parameter_grid,\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    scoring='accuracy',\n",
    ")\n",
    "\n",
    "grid_search.fit(x_train_upsampled, y_train_upsampled)\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "print(f\"Optimal parameters: {best_params}\")\n",
    "\n",
    "gb_optimal = GradientBoostingClassifier(**best_params, random_state=42)\n",
    "gb_optimal.fit(x_train_upsampled, y_train_upsampled)\n",
    "\n",
    "y_pred = gb_optimal.predict(x_validation)\n",
    "y_pred_proba = gb_optimal.predict_proba(x_validation)[:, 1]\n",
    "\n",
    "acc = metrics.accuracy_score(y_validation, y_pred)\n",
    "precision = metrics.precision_score(y_validation, y_pred)\n",
    "recall = metrics.recall_score(y_validation, y_pred)\n",
    "f1 = metrics.f1_score(y_validation, y_pred)\n",
    "auc = metrics.roc_auc_score(y_validation, y_pred_proba)\n",
    "fpr, tpr, _ = metrics.roc_curve(y_validation, y_pred_proba)\n",
    "\n",
    "results = [{\n",
    "    'Label': '8:1:1',\n",
    "    'Accuracy': \"{:.3f}\".format(acc),\n",
    "    'Precision': \"{:.3f}\".format(precision),\n",
    "    'Recall': \"{:.3f}\".format(recall),\n",
    "    'ROC AUC': \"{:.3f}\".format(auc),\n",
    "    'F1 Score': \"{:.3f}\".format(f1)\n",
    "}]\n",
    "\n",
    "df_metrics = pd.DataFrame(results)\n",
    "print(df_metrics[['Label', 'Accuracy', 'Precision', 'Recall', 'ROC AUC', 'F1 Score']])\n",
    "\n",
    "# Plot the ROC Curve\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "ax.plot(fpr, tpr, label=f\"ROC AUC = {auc:.3f}\", color='b')\n",
    "ax.plot([0, 1], [0, 1], 'k--', lw=2, alpha=0.7)\n",
    "ax.legend(loc=\"lower right\")\n",
    "ax.set_xlabel('False Positive Rate')\n",
    "ax.set_ylabel('True Positive Rate')\n",
    "ax.set_title(\"ROC Curve for Optimized Gradient Boosting Model\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_grid = {\n",
    "    'penalty': ['elasticnet'],\n",
    "    'solver': ['saga'],\n",
    "    'C': [10, 50, 100,300, 500],\n",
    "    'l1_ratio': [0.0001, 0.001, 0.01],\n",
    "    'max_iter': [1000],\n",
    "    'tol': [1e-4, 1e-3, 1e-2],\n",
    "    'class_weight': [None, 'balanced'],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    LogisticRegression(random_state=42),\n",
    "    param_grid=parameter_grid,\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    scoring='accuracy'\n",
    ")\n",
    "grid_search.fit(x_train_upsampled, y_train_upsampled)\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "print(f\"Optimal parameters: {best_params}\")\n",
    "\n",
    "best_classifier = LogisticRegression(**best_params, random_state=42)\n",
    "best_classifier.fit(x_train_upsampled, y_train_upsampled)\n",
    "\n",
    "y_pred = best_classifier.predict(x_validation)\n",
    "y_pred_proba = best_classifier.predict_proba(x_validation)[:, 1]\n",
    "\n",
    "acc = metrics.accuracy_score(y_validation, y_pred)\n",
    "precision = metrics.precision_score(y_validation, y_pred)\n",
    "recall = metrics.recall_score(y_validation, y_pred)\n",
    "f1 = metrics.f1_score(y_validation, y_pred)\n",
    "auc = metrics.roc_auc_score(y_validation, y_pred_proba)\n",
    "fpr, tpr, _ = metrics.roc_curve(y_validation, y_pred_proba)\n",
    "\n",
    "results = [{\n",
    "    'Label': '8:1:1',\n",
    "    'Accuracy': \"{:.3f}\".format(acc),\n",
    "    'Precision': \"{:.3f}\".format(precision),\n",
    "    'Recall': \"{:.3f}\".format(recall),\n",
    "    'ROC AUC': \"{:.3f}\".format(auc),\n",
    "    'F1 Score': \"{:.3f}\".format(f1),\n",
    "    'fpr': fpr,\n",
    "    'tpr': tpr\n",
    "}]\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "print(df[['Label', 'Accuracy', 'Precision', 'Recall', 'ROC AUC', 'F1 Score']])\n",
    "\n",
    "\n",
    "# Plot the ROC Curve\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "ax.plot(fpr, tpr, label=f\"ROC AUC = {auc:.3f}\", color='b')\n",
    "ax.plot([0, 1], [0, 1], 'k--', lw=2, alpha=0.7)\n",
    "ax.legend(loc=\"lower right\")\n",
    "ax.set_xlabel('False Positive Rate')\n",
    "ax.set_ylabel('True Positive Rate')\n",
    "ax.set_title(\"ROC Curve for Optimized ElasticNet Logistic Regression\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 0.8\n",
    "test_size = 0.1\n",
    "validation_size = 0.1\n",
    "\n",
    "parameter_grid = {\n",
    "    'hidden_layer_sizes': [\n",
    "        (128,),\n",
    "        (64,),\n",
    "        (128, 64),\n",
    "        (64, 32),\n",
    "        (128, 64, 32)\n",
    "    ],\n",
    "    'activation': ['relu', 'tanh', 'logistic'],\n",
    "    'learning_rate_init': [0.001, 0.01, 0.1],\n",
    "    'alpha': [ 0.001, 0.01, 0.1]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=MLPClassifier(\n",
    "        solver='adam',\n",
    "        max_iter=200,\n",
    "        random_state=42\n",
    "    ),\n",
    "    param_grid=parameter_grid,\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    scoring='accuracy',\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "grid_search.fit(x_train_upsampled, y_train_upsampled)\n",
    "\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "print(f\"Optimal parameters: {best_params}\")\n",
    "\n",
    "\n",
    "mlp_optimal = MLPClassifier(\n",
    "    hidden_layer_sizes=best_params['hidden_layer_sizes'],\n",
    "    activation=best_params['activation'],\n",
    "    solver='adam',\n",
    "    learning_rate_init=best_params['learning_rate_init'],\n",
    "    alpha=best_params['alpha'],\n",
    "    max_iter=300,\n",
    "    random_state=42\n",
    ")\n",
    "mlp_optimal.fit(x_train_upsampled, y_train_upsampled)\n",
    "\n",
    "\n",
    "y_pred = mlp_optimal.predict(x_validation)\n",
    "y_pred_proba = mlp_optimal.predict_proba(x_validation)[:, 1]\n",
    "\n",
    "\n",
    "acc = metrics.accuracy_score(y_validation, y_pred)\n",
    "precision = metrics.precision_score(y_validation, y_pred, zero_division=0)\n",
    "recall = metrics.recall_score(y_validation, y_pred, zero_division=0)\n",
    "f1 = metrics.f1_score(y_validation, y_pred, zero_division=0)\n",
    "auc = metrics.roc_auc_score(y_validation, y_pred_proba)\n",
    "fpr, tpr, _ = metrics.roc_curve(y_validation, y_pred_proba)\n",
    "\n",
    "\n",
    "results = [{\n",
    "    'Label': '8:1:1',\n",
    "    'Accuracy': \"{:.3f}\".format(acc),\n",
    "    'Precision': \"{:.3f}\".format(precision),\n",
    "    'Recall': \"{:.3f}\".format(recall),\n",
    "    'ROC AUC': \"{:.3f}\".format(auc),\n",
    "    'F1 Score': \"{:.3f}\".format(f1),\n",
    "    'fpr': fpr,\n",
    "    'tpr': tpr\n",
    "}]\n",
    "\n",
    "\n",
    "df_metrics = pd.DataFrame(results)\n",
    "print(df_metrics[['Label', 'Accuracy', 'Precision', 'Recall', 'ROC AUC', 'F1 Score']])\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,  6))\n",
    "ax.plot(fpr, tpr, label=f\"ROC AUC = {auc:.3f}\", color='b')\n",
    "\n",
    "ax.plot([0, 1], [0, 1], 'k--', lw=2, alpha=0.7)\n",
    "ax.legend(loc=\"lower right\")\n",
    "ax.set_xlabel('False Positive Rate')\n",
    "ax.set_ylabel('True Positive Rate')\n",
    "ax.set_title(\"ROC Curve for Optimized MLP Model\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only nn.TransformerEncoder and nn.TransformerEncoderLayer\n",
    "train_size = 0.8\n",
    "test_size = 0.1\n",
    "validation_size = 0.1\n",
    "\n",
    "class SimpleTransformerClassifier(nn.Module):\n",
    "    def __init__(self, n_features, embed_dim=32, nhead=4, num_layers=2, dim_feedforward=64, activation='relu'):\n",
    "        super().__init__()\n",
    "        self.n_features = n_features\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        self.feature_proj = nn.Linear(n_features, embed_dim)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            batch_first=True,\n",
    "            activation=activation\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        self.fc = nn.Linear(embed_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        proj = self.feature_proj(x)\n",
    "        proj = proj.view(batch_size, self.n_features, self.embed_dim)\n",
    "\n",
    "        encoded = self.transformer_encoder(proj)\n",
    "\n",
    "        pooled = encoded.mean(dim=1)\n",
    "        logit = self.fc(pooled)\n",
    "        prob = self.sigmoid(logit)\n",
    "        return prob\n",
    "\n",
    "\n",
    "class PyTorchTransformerClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self,\n",
    "                 embed_dim=32, nhead=4, num_layers=2, dim_feedforward=64,\n",
    "                 learning_rate=0.001, max_iter=200, device='cpu', activation='relu'):\n",
    "        self.embed_dim = embed_dim\n",
    "        self.nhead = nhead\n",
    "        self.num_layers = num_layers\n",
    "        self.dim_feedforward = dim_feedforward\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iter = max_iter\n",
    "        self.device = device\n",
    "        self.activation = activation\n",
    "        self.model_ = None\n",
    "        self.n_features_ = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.n_features_ = X.shape[1]\n",
    "        X_tensor = torch.tensor(X, dtype=torch.float32).to(self.device)\n",
    "        y_tensor = torch.tensor(y, dtype=torch.float32).view(-1,1).to(self.device)\n",
    "\n",
    "        self.model_ = SimpleTransformerClassifier(\n",
    "            n_features=self.n_features_,\n",
    "            embed_dim=self.embed_dim,\n",
    "            nhead=self.nhead,\n",
    "            num_layers=self.num_layers,\n",
    "            dim_feedforward=self.dim_feedforward,\n",
    "            activation=self.activation\n",
    "        ).to(self.device)\n",
    "\n",
    "        self.model_.feature_proj = nn.Linear(self.n_features_, self.n_features_*self.embed_dim).to(self.device)\n",
    "\n",
    "        optimizer = optim.Adam(self.model_.parameters(), lr=self.learning_rate)\n",
    "        criterion = nn.BCELoss()\n",
    "\n",
    "        self.model_.train()\n",
    "        for epoch in range(self.max_iter):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = self.model_(X_tensor)\n",
    "            loss = criterion(outputs, y_tensor)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        self.model_.eval()\n",
    "        with torch.no_grad():\n",
    "            X_tensor = torch.tensor(X, dtype=torch.float32).to(self.device)\n",
    "            pred = self.model_(X_tensor).cpu().numpy()\n",
    "        return np.hstack([1 - pred, pred])\n",
    "\n",
    "    def predict(self, X):\n",
    "        proba = self.predict_proba(X)[:, 1]\n",
    "        return (proba >= 0.5).astype(int)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "transformer_optimal = PyTorchTransformerClassifier(\n",
    "    embed_dim=16,\n",
    "    nhead=4,\n",
    "    num_layers=1,\n",
    "    dim_feedforward=64,\n",
    "    learning_rate=1e-3,\n",
    "    max_iter=200,\n",
    "    activation='relu',\n",
    "    device='cpu'\n",
    ")\n",
    "\n",
    "x_train_upsampled = x_train_upsampled.values if hasattr(x_train_upsampled, 'values') else x_train_upsampled\n",
    "x_validation = x_validation.values if hasattr(x_validation, 'values') else x_validation\n",
    "y_train_upsampled = y_train_upsampled.values if hasattr(y_train_upsampled, 'values') else y_train_upsampled\n",
    "y_validation = y_validation.values if hasattr(y_validation, 'values') else y_validation\n",
    "\n",
    "transformer_optimal.fit(x_train_upsampled, y_train_upsampled)\n",
    "\n",
    "y_pred = transformer_optimal.predict(x_validation)\n",
    "y_pred_proba = transformer_optimal.predict_proba(x_validation)[:, 1]\n",
    "\n",
    "acc = metrics.accuracy_score(y_validation, y_pred)\n",
    "precision = metrics.precision_score(y_validation, y_pred, zero_division=0)\n",
    "recall = metrics.recall_score(y_validation, y_pred, zero_division=0)\n",
    "f1 = metrics.f1_score(y_validation, y_pred, zero_division=0)\n",
    "auc = metrics.roc_auc_score(y_validation, y_pred_proba)\n",
    "fpr, tpr, _ = metrics.roc_curve(y_validation, y_pred_proba)\n",
    "\n",
    "results = [{\n",
    "    'Label': \"8:1:1\",\n",
    "    'Accuracy': \"{:.3f}\".format(acc),\n",
    "    'Precision': \"{:.3f}\".format(precision),\n",
    "    'Recall': \"{:.3f}\".format(recall),\n",
    "    'ROC AUC': \"{:.3f}\".format(auc),\n",
    "    'F1 Score': \"{:.3f}\".format(f1),\n",
    "    'fpr': fpr,\n",
    "    'tpr': tpr\n",
    "}]\n",
    "\n",
    "df_metrics = pd.DataFrame(results)\n",
    "print(df_metrics[['Label', 'Accuracy', 'Precision', 'Recall', 'ROC AUC', 'F1 Score']])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.plot(fpr, tpr, label=f\"ROC AUC = {auc:.3f}\", color='b')\n",
    "ax.plot([0, 1], [0, 1], 'k--', lw=2, alpha=0.7)\n",
    "ax.legend(loc=\"lower right\")\n",
    "ax.set_xlabel('False Positive Rate')\n",
    "ax.set_ylabel('True Positive Rate')\n",
    "ax.set_title(\"ROC Curve for Optimized Transformer Model\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 0.8\n",
    "test_size = 0.1\n",
    "validation_size = 0.1\n",
    "\n",
    "class UnivariateMLP(nn.Module):\n",
    "    def __init__(self, hidden_dim=32, activation='relu'):\n",
    "        super().__init__()\n",
    "        act_map = {\n",
    "            'relu': nn.ReLU(),\n",
    "            'tanh': nn.Tanh(),\n",
    "            'gelu': nn.GELU()\n",
    "        }\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(1, hidden_dim),\n",
    "            act_map.get(activation, nn.ReLU()),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            act_map.get(activation, nn.ReLU()),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, 1]\n",
    "        return self.net(x)\n",
    "\n",
    "class KANModel(nn.Module):\n",
    "    def __init__(self, n_features, Q=2, hidden_dim=32, activation='relu'):\n",
    "        super().__init__()\n",
    "        self.n_features = n_features\n",
    "        self.Q = Q\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.psi_funcs = nn.ModuleList([\n",
    "            nn.ModuleList([UnivariateMLP(hidden_dim, activation) for _ in range(n_features)])\n",
    "            for _ in range(Q)\n",
    "        ])\n",
    "\n",
    "        # phi_q\n",
    "        self.phi_funcs = nn.ModuleList([\n",
    "            UnivariateMLP(hidden_dim, activation) for _ in range(Q)\n",
    "        ])\n",
    "\n",
    "        self.fc = nn.Linear(Q, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, n_features]\n",
    "        batch_size = x.size(0)\n",
    "        z_list = []\n",
    "        for q in range(self.Q):\n",
    "            sum_over_p = 0\n",
    "            for p in range(self.n_features):\n",
    "                xp = x[:, p:p+1]\n",
    "                val = self.psi_funcs[q][p](xp)\n",
    "                sum_over_p = sum_over_p + val\n",
    "            phi_val = self.phi_funcs[q](sum_over_p) # [batch_size,1]\n",
    "            z_list.append(phi_val)\n",
    "\n",
    "        Z = torch.cat(z_list, dim=1)\n",
    "\n",
    "        logit = self.fc(Z) # [batch_size,1]\n",
    "        prob = self.sigmoid(logit)\n",
    "        return prob\n",
    "\n",
    "\n",
    "class PyTorchKANClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, Q=2, hidden_dim=32, activation='relu',\n",
    "                 learning_rate=0.001, max_iter=100, device='cpu'):\n",
    "        self.Q = Q\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.activation = activation\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iter = max_iter\n",
    "        self.device = device\n",
    "        self.model_ = None\n",
    "        self.n_features_ = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.n_features_ = X.shape[1]\n",
    "        X_tensor = torch.tensor(X, dtype=torch.float32).to(self.device)\n",
    "        y_tensor = torch.tensor(y, dtype=torch.float32).view(-1,1).to(self.device)\n",
    "\n",
    "        self.model_ = KANModel(\n",
    "            n_features=self.n_features_,\n",
    "            Q=self.Q,\n",
    "            hidden_dim=self.hidden_dim,\n",
    "            activation=self.activation\n",
    "        ).to(self.device)\n",
    "\n",
    "        optimizer = optim.Adam(self.model_.parameters(), lr=self.learning_rate)\n",
    "        criterion = nn.BCELoss()\n",
    "\n",
    "        self.model_.train()\n",
    "        for epoch in range(self.max_iter):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = self.model_(X_tensor)\n",
    "            loss = criterion(outputs, y_tensor)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        self.model_.eval()\n",
    "        with torch.no_grad():\n",
    "            X_tensor = torch.tensor(X, dtype=torch.float32).to(self.device)\n",
    "            pred = self.model_(X_tensor).cpu().numpy() # [batch_size,1]\n",
    "        return np.hstack([1 - pred, pred])\n",
    "\n",
    "    def predict(self, X):\n",
    "        proba = self.predict_proba(X)[:, 1]\n",
    "        return (proba >= 0.5).astype(int)\n",
    "\n",
    "\n",
    "kan_optimal = PyTorchKANClassifier(\n",
    "    Q=2,\n",
    "    hidden_dim=64,\n",
    "    activation='relu',\n",
    "    learning_rate=1e-3,\n",
    "    max_iter=1000,\n",
    "    device='cpu'\n",
    ")\n",
    "\n",
    "x_train_upsampled = x_train_upsampled.values if hasattr(x_train_upsampled, 'values') else x_train_upsampled\n",
    "x_validation = x_validation.values if hasattr(x_validation, 'values') else x_validation\n",
    "y_train_upsampled = y_train_upsampled.values if hasattr(y_train_upsampled, 'values') else y_train_upsampled\n",
    "y_validation = y_validation.values if hasattr(y_validation, 'values') else y_validation\n",
    "\n",
    "kan_optimal.fit(x_train_upsampled, y_train_upsampled)\n",
    "\n",
    "\n",
    "y_pred = kan_optimal.predict(x_validation)\n",
    "y_pred_proba = kan_optimal.predict_proba(x_validation)[:, 1]\n",
    "\n",
    "acc = metrics.accuracy_score(y_validation, y_pred)\n",
    "precision = metrics.precision_score(y_validation, y_pred, zero_division=0)\n",
    "recall = metrics.recall_score(y_validation, y_pred, zero_division=0)\n",
    "f1 = metrics.f1_score(y_validation, y_pred, zero_division=0)\n",
    "auc = metrics.roc_auc_score(y_validation, y_pred_proba)\n",
    "fpr, tpr, _ = metrics.roc_curve(y_validation, y_pred_proba)\n",
    "\n",
    "results = [{\n",
    "    'Label': \"8:1:1\",\n",
    "    'Accuracy': \"{:.3f}\".format(acc),\n",
    "    'Precision': \"{:.3f}\".format(precision),\n",
    "    'Recall': \"{:.3f}\".format(recall),\n",
    "    'ROC AUC': \"{:.3f}\".format(auc),\n",
    "    'F1 Score': \"{:.3f}\".format(f1),\n",
    "    'fpr': fpr,\n",
    "    'tpr': tpr\n",
    "}]\n",
    "\n",
    "df_metrics = pd.DataFrame(results)\n",
    "print(df_metrics[['Label', 'Accuracy', 'Precision', 'Recall', 'ROC AUC', 'F1 Score']])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.plot(fpr, tpr, label=f\"ROC AUC = {auc:.3f}\", color='b')\n",
    "ax.plot([0, 1], [0, 1], 'k--', lw=2, alpha=0.7)\n",
    "ax.legend(loc=\"lower right\")\n",
    "ax.set_xlabel('False Positive Rate')\n",
    "ax.set_ylabel('True Positive Rate')\n",
    "ax.set_title(\"ROC Curve for Optimized KAN Model\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 0.8\n",
    "test_size = 0.1\n",
    "validation_size = 0.1\n",
    "\n",
    "class SimpleRNNClassifier(nn.Module):\n",
    "    def __init__(self, n_features, hidden_size=32, num_layers=1, activation='relu'):\n",
    "        super().__init__()\n",
    "        self.n_features = n_features\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "\n",
    "        self.rnn = nn.RNN(input_size=1, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "\n",
    "\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "\n",
    "        act_map = {\n",
    "            'relu': nn.ReLU(),\n",
    "            'tanh': nn.Tanh(),\n",
    "            'gelu': nn.GELU()\n",
    "        }\n",
    "        self.activation = act_map.get(activation, nn.ReLU())\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        batch_size = x.size(0)\n",
    "        seq_len = self.n_features\n",
    "        # reshape x => [batch_size, seq_len, 1]\n",
    "        x_seq = x.view(batch_size, seq_len, 1)\n",
    "\n",
    "\n",
    "        rnn_out, h = self.rnn(x_seq)  # rnn_out: [batch_size, seq_len, hidden_size]\n",
    "\n",
    "\n",
    "        out = rnn_out[:, -1, :]\n",
    "\n",
    "        logit = self.fc(out)\n",
    "        prob = self.sigmoid(logit)\n",
    "        return prob\n",
    "\n",
    "\n",
    "class PyTorchRNNClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, hidden_size=32, num_layers=1, activation='relu',\n",
    "                 learning_rate=0.001, max_iter=200, device='cpu'):\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.activation = activation\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iter = max_iter\n",
    "        self.device = device\n",
    "        self.model_ = None\n",
    "        self.n_features_ = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        if hasattr(X, 'values'):\n",
    "            X = X.values\n",
    "        if hasattr(y, 'values'):\n",
    "            y = y.values\n",
    "\n",
    "        self.n_features_ = X.shape[1]\n",
    "        X_tensor = torch.tensor(X, dtype=torch.float32).to(self.device)\n",
    "        y_tensor = torch.tensor(y, dtype=torch.float32).view(-1,1).to(self.device)\n",
    "\n",
    "        self.model_ = SimpleRNNClassifier(\n",
    "            n_features=self.n_features_,\n",
    "            hidden_size=self.hidden_size,\n",
    "            num_layers=self.num_layers,\n",
    "            activation=self.activation\n",
    "        ).to(self.device)\n",
    "\n",
    "        optimizer = optim.Adam(self.model_.parameters(), lr=self.learning_rate)\n",
    "        criterion = nn.BCELoss()\n",
    "\n",
    "        self.model_.train()\n",
    "        for epoch in range(self.max_iter):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = self.model_(X_tensor)\n",
    "            loss = criterion(outputs, y_tensor)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        if hasattr(X, 'values'):\n",
    "            X = X.values\n",
    "        self.model_.eval()\n",
    "        with torch.no_grad():\n",
    "            X_tensor = torch.tensor(X, dtype=torch.float32).to(self.device)\n",
    "            pred = self.model_(X_tensor).cpu().numpy()\n",
    "        return np.hstack([1 - pred, pred])\n",
    "\n",
    "    def predict(self, X):\n",
    "        proba = self.predict_proba(X)[:, 1]\n",
    "        return (proba >= 0.5).astype(int)\n",
    "\n",
    "\n",
    "rnn_optimal = PyTorchRNNClassifier(\n",
    "    hidden_size=64,\n",
    "    num_layers=2,\n",
    "    activation='relu',\n",
    "    learning_rate=1e-3,\n",
    "    max_iter=200,\n",
    "    device='cpu'\n",
    ")\n",
    "\n",
    "x_train_upsampled = x_train_upsampled.values if hasattr(x_train_upsampled, 'values') else x_train_upsampled\n",
    "x_validation = x_validation.values if hasattr(x_validation, 'values') else x_validation\n",
    "y_train_upsampled = y_train_upsampled.values if hasattr(y_train_upsampled, 'values') else y_train_upsampled\n",
    "y_validation = y_validation.values if hasattr(y_validation, 'values') else y_validation\n",
    "\n",
    "rnn_optimal.fit(x_train_upsampled, y_train_upsampled)\n",
    "\n",
    "y_pred = rnn_optimal.predict(x_validation)\n",
    "y_pred_proba = rnn_optimal.predict_proba(x_validation)[:, 1]\n",
    "\n",
    "acc = metrics.accuracy_score(y_validation, y_pred)\n",
    "precision = metrics.precision_score(y_validation, y_pred, zero_division=0)\n",
    "recall = metrics.recall_score(y_validation, y_pred, zero_division=0)\n",
    "f1 = metrics.f1_score(y_validation, y_pred, zero_division=0)\n",
    "auc = metrics.roc_auc_score(y_validation, y_pred_proba)\n",
    "fpr, tpr, _ = metrics.roc_curve(y_validation, y_pred_proba)\n",
    "\n",
    "results = [{\n",
    "    'Label': \"8:1:1\",\n",
    "    'Accuracy': \"{:.3f}\".format(acc),\n",
    "    'Precision': \"{:.3f}\".format(precision),\n",
    "    'Recall': \"{:.3f}\".format(recall),\n",
    "    'ROC AUC': \"{:.3f}\".format(auc),\n",
    "    'F1 Score': \"{:.3f}\".format(f1),\n",
    "    'fpr': fpr,\n",
    "    'tpr': tpr\n",
    "}]\n",
    "\n",
    "df_metrics = pd.DataFrame(results)\n",
    "print(df_metrics[['Label', 'Accuracy', 'Precision', 'Recall', 'ROC AUC', 'F1 Score']])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.plot(fpr, tpr, label=f\"ROC AUC = {auc:.3f}\", color='b')\n",
    "ax.plot([0, 1], [0, 1], 'k--', lw=2, alpha=0.7)\n",
    "ax.legend(loc=\"lower right\")\n",
    "ax.set_xlabel('False Positive Rate')\n",
    "ax.set_ylabel('True Positive Rate')\n",
    "ax.set_title(\"ROC Curve for RNN Model\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 0.8\n",
    "test_size = 0.1\n",
    "validation_size = 0.1\n",
    "\n",
    "class SimpleCNNClassifier(nn.Module):\n",
    "    def __init__(self, n_features, num_filters=32, kernel_size=3, activation='relu'):\n",
    "        super().__init__()\n",
    "        self.n_features = n_features\n",
    "        self.num_filters = num_filters\n",
    "        self.kernel_size = kernel_size\n",
    "\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=num_filters, kernel_size=kernel_size, padding='same')\n",
    "\n",
    "\n",
    "        act_map = {\n",
    "            'relu': nn.ReLU(),\n",
    "            'tanh': nn.Tanh(),\n",
    "            'gelu': nn.GELU()\n",
    "        }\n",
    "        self.activation = act_map.get(activation, nn.ReLU())\n",
    "\n",
    "\n",
    "        self.pool = nn.AdaptiveAvgPool1d(output_size=1)\n",
    "\n",
    "        self.fc = nn.Linear(num_filters, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, n_features]\n",
    "        batch_size = x.size(0)\n",
    "        x = x.view(batch_size, 1, self.n_features)  # [batch_size, 1, n_features]\n",
    "\n",
    "        out = self.conv1(x)  # [batch_size, num_filters, n_features]\n",
    "        out = self.activation(out)\n",
    "        out = self.pool(out) # [batch_size, num_filters, 1]\n",
    "\n",
    "        out = out.squeeze(-1) # [batch_size, num_filters]\n",
    "        logit = self.fc(out)  # [batch_size, 1]\n",
    "        prob = self.sigmoid(logit)\n",
    "        return prob\n",
    "\n",
    "\n",
    "class PyTorchCNNClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, num_filters=32, kernel_size=3, activation='relu',\n",
    "                 learning_rate=0.001, max_iter=100, batch_size=64, device='cpu'):\n",
    "        self.num_filters = num_filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.activation = activation\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iter = max_iter\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "        self.model_ = None\n",
    "        self.n_features_ = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        if hasattr(X, 'values'):\n",
    "            X = X.values\n",
    "        if hasattr(y, 'values'):\n",
    "            y = y.values\n",
    "\n",
    "        self.n_features_ = X.shape[1]\n",
    "        X_tensor = torch.tensor(X, dtype=torch.float32).to(self.device)\n",
    "        y_tensor = torch.tensor(y, dtype=torch.float32).view(-1,1).to(self.device)\n",
    "\n",
    "        self.model_ = SimpleCNNClassifier(\n",
    "            n_features=self.n_features_,\n",
    "            num_filters=self.num_filters,\n",
    "            kernel_size=self.kernel_size,\n",
    "            activation=self.activation\n",
    "        ).to(self.device)\n",
    "\n",
    "        optimizer = optim.Adam(self.model_.parameters(), lr=self.learning_rate)\n",
    "        criterion = nn.BCELoss()\n",
    "\n",
    "        self.model_.train()\n",
    "        for epoch in range(self.max_iter):\n",
    "            for i in range(0, X_tensor.size(0), self.batch_size):\n",
    "                batch_x = X_tensor[i:i+self.batch_size]\n",
    "                batch_y = y_tensor[i:i+self.batch_size]\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model_(batch_x)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            if (epoch+1) % 10 == 0:\n",
    "                print(f\"Epoch [{epoch+1}/{self.max_iter}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        if hasattr(X, 'values'):\n",
    "            X = X.values\n",
    "        self.model_.eval()\n",
    "        with torch.no_grad():\n",
    "            X_tensor = torch.tensor(X, dtype=torch.float32).to(self.device)\n",
    "            pred = self.model_(X_tensor).cpu().numpy()\n",
    "        return np.hstack([1 - pred, pred])\n",
    "\n",
    "    def predict(self, X):\n",
    "        proba = self.predict_proba(X)[:, 1]\n",
    "        return (proba >= 0.5).astype(int)\n",
    "\n",
    "\n",
    "cnn_clf = PyTorchCNNClassifier(\n",
    "    num_filters=32,\n",
    "    kernel_size=3,\n",
    "    activation='relu',\n",
    "    learning_rate=1e-3,\n",
    "    max_iter=200,\n",
    "    batch_size=64,\n",
    "    device='cpu'\n",
    ")\n",
    "\n",
    "x_train_upsampled = x_train_upsampled.values if hasattr(x_train_upsampled, 'values') else x_train_upsampled\n",
    "x_validation = x_validation.values if hasattr(x_validation, 'values') else x_validation\n",
    "y_train_upsampled = y_train_upsampled.values if hasattr(y_train_upsampled, 'values') else y_train_upsampled\n",
    "y_validation = y_validation.values if hasattr(y_validation, 'values') else y_validation\n",
    "\n",
    "cnn_clf.fit(x_train_upsampled, y_train_upsampled)\n",
    "\n",
    "y_pred = cnn_clf.predict(x_validation)\n",
    "y_pred_proba = cnn_clf.predict_proba(x_validation)[:, 1]\n",
    "\n",
    "acc = metrics.accuracy_score(y_validation, y_pred)\n",
    "precision = metrics.precision_score(y_validation, y_pred, zero_division=0)\n",
    "recall = metrics.recall_score(y_validation, y_pred, zero_division=0)\n",
    "f1 = metrics.f1_score(y_validation, y_pred, zero_division=0)\n",
    "auc = metrics.roc_auc_score(y_validation, y_pred_proba)\n",
    "fpr, tpr, _ = metrics.roc_curve(y_validation, y_pred_proba)\n",
    "\n",
    "results = [{\n",
    "    'Label': \"8:1:1\",\n",
    "    'Accuracy': \"{:.3f}\".format(acc),\n",
    "    'Precision': \"{:.3f}\".format(precision),\n",
    "    'Recall': \"{:.3f}\".format(recall),\n",
    "    'ROC AUC': \"{:.3f}\".format(auc),\n",
    "    'F1 Score': \"{:.3f}\".format(f1),\n",
    "    'fpr': fpr,\n",
    "    'tpr': tpr\n",
    "}]\n",
    "\n",
    "df_metrics = pd.DataFrame(results)\n",
    "print(df_metrics[['Label', 'Accuracy', 'Precision', 'Recall', 'ROC AUC', 'F1 Score']])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.plot(fpr, tpr, label=f\"ROC AUC = {auc:.3f}\", color='b')\n",
    "ax.plot([0, 1], [0, 1], 'k--', lw=2, alpha=0.7)\n",
    "ax.legend(loc=\"lower right\")\n",
    "ax.set_xlabel('False Positive Rate')\n",
    "ax.set_ylabel('True Positive Rate')\n",
    "ax.set_title(\"ROC Curve for CNN Model\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 0.8\n",
    "test_size = 0.1\n",
    "validation_size = 0.1\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, activation=nn.ReLU()):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size, padding='same', bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        self.activation = activation\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size, padding='same', bias=False)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels, kernel_size=1, bias=False),\n",
    "                nn.BatchNorm1d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.activation(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        shortcut = self.shortcut(x)\n",
    "\n",
    "        out = out + shortcut\n",
    "        out = self.activation(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class SimpleResNetClassifier(nn.Module):\n",
    "    def __init__(self, n_features, num_blocks=2, hidden_dim=32, kernel_size=3, activation='relu'):\n",
    "        super().__init__()\n",
    "        act_map = {\n",
    "            'relu': nn.ReLU(),\n",
    "            'tanh': nn.Tanh(),\n",
    "            'gelu': nn.GELU()\n",
    "        }\n",
    "        act = act_map.get(activation, nn.ReLU())\n",
    "\n",
    "        self.conv_in = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=1, out_channels=hidden_dim, kernel_size=kernel_size, padding='same', bias=False),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            act\n",
    "        )\n",
    "\n",
    "        layers = []\n",
    "        in_channels = hidden_dim\n",
    "        out_channels = hidden_dim\n",
    "        for i in range(num_blocks):\n",
    "            layers.append(BasicBlock(in_channels, out_channels, kernel_size=kernel_size, activation=act))\n",
    "            in_channels = out_channels\n",
    "\n",
    "        self.blocks = nn.Sequential(*layers)\n",
    "\n",
    "        self.pool = nn.AdaptiveAvgPool1d(output_size=1)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, n_features]\n",
    "        batch_size, n_features = x.size()\n",
    "        x = x.view(batch_size, 1, n_features)  # [batch_size, 1, n_features]\n",
    "\n",
    "        out = self.conv_in(x)    # [batch_size, hidden_dim, n_features]\n",
    "        out = self.blocks(out)   # [batch_size, hidden_dim, n_features]\n",
    "\n",
    "        out = self.pool(out)     # [batch_size, hidden_dim, 1]\n",
    "        out = out.squeeze(-1)    # [batch_size, hidden_dim]\n",
    "\n",
    "        logit = self.fc(out)     # [batch_size, 1]\n",
    "        prob = self.sigmoid(logit)\n",
    "        return prob\n",
    "\n",
    "\n",
    "class PyTorchResNetClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, num_blocks=2, hidden_dim=32, kernel_size=3, activation='relu',\n",
    "                 learning_rate=0.001, max_iter=100, batch_size=64, device='cpu'):\n",
    "        self.num_blocks = num_blocks\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.kernel_size = kernel_size\n",
    "        self.activation = activation\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iter = max_iter\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "        self.model_ = None\n",
    "        self.n_features_ = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        if hasattr(X, 'values'):\n",
    "            X = X.values\n",
    "        if hasattr(y, 'values'):\n",
    "            y = y.values\n",
    "\n",
    "        self.n_features_ = X.shape[1]\n",
    "        X_tensor = torch.tensor(X, dtype=torch.float32).to(self.device)\n",
    "        y_tensor = torch.tensor(y, dtype=torch.float32).view(-1,1).to(self.device)\n",
    "\n",
    "        self.model_ = SimpleResNetClassifier(\n",
    "            n_features=self.n_features_,\n",
    "            num_blocks=self.num_blocks,\n",
    "            hidden_dim=self.hidden_dim,\n",
    "            kernel_size=self.kernel_size,\n",
    "            activation=self.activation\n",
    "        ).to(self.device)\n",
    "\n",
    "        optimizer = optim.Adam(self.model_.parameters(), lr=self.learning_rate)\n",
    "        criterion = nn.BCELoss()\n",
    "\n",
    "        self.model_.train()\n",
    "        for epoch in range(self.max_iter):\n",
    "            for i in range(0, X_tensor.size(0), self.batch_size):\n",
    "                batch_x = X_tensor[i:i+self.batch_size]\n",
    "                batch_y = y_tensor[i:i+self.batch_size]\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model_(batch_x)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            if (epoch+1) % 10 == 0:\n",
    "                print(f\"Epoch [{epoch+1}/{self.max_iter}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        if hasattr(X, 'values'):\n",
    "            X = X.values\n",
    "        self.model_.eval()\n",
    "        with torch.no_grad():\n",
    "            X_tensor = torch.tensor(X, dtype=torch.float32).to(self.device)\n",
    "            pred = self.model_(X_tensor).cpu().numpy()\n",
    "        return np.hstack([1 - pred, pred])\n",
    "\n",
    "    def predict(self, X):\n",
    "        proba = self.predict_proba(X)[:, 1]\n",
    "        return (proba >= 0.5).astype(int)\n",
    "\n",
    "\n",
    "resnet_clf = PyTorchResNetClassifier(\n",
    "    num_blocks=2,\n",
    "    hidden_dim=32,\n",
    "    kernel_size=3,\n",
    "    activation='relu',\n",
    "    learning_rate=1e-3,\n",
    "    max_iter=200,\n",
    "    batch_size=64,\n",
    "    device='cpu'\n",
    ")\n",
    "\n",
    "\n",
    "x_train_upsampled = x_train_upsampled.values if hasattr(x_train_upsampled, 'values') else x_train_upsampled\n",
    "x_validation = x_validation.values if hasattr(x_validation, 'values') else x_validation\n",
    "y_train_upsampled = y_train_upsampled.values if hasattr(y_train_upsampled, 'values') else y_train_upsampled\n",
    "y_validation = y_validation.values if hasattr(y_validation, 'values') else y_validation\n",
    "\n",
    "resnet_clf.fit(x_train_upsampled, y_train_upsampled)\n",
    "\n",
    "y_pred = resnet_clf.predict(x_validation)\n",
    "y_pred_proba = resnet_clf.predict_proba(x_validation)[:, 1]\n",
    "\n",
    "acc = metrics.accuracy_score(y_validation, y_pred)\n",
    "precision = metrics.precision_score(y_validation, y_pred, zero_division=0)\n",
    "recall = metrics.recall_score(y_validation, y_pred, zero_division=0)\n",
    "f1 = metrics.f1_score(y_validation, y_pred, zero_division=0)\n",
    "auc = metrics.roc_auc_score(y_validation, y_pred_proba)\n",
    "fpr, tpr, _ = metrics.roc_curve(y_validation, y_pred_proba)\n",
    "\n",
    "results = [{\n",
    "    'Label': \"8:1:1\",\n",
    "    'Accuracy': \"{:.3f}\".format(acc),\n",
    "    'Precision': \"{:.3f}\".format(precision),\n",
    "    'Recall': \"{:.3f}\".format(recall),\n",
    "    'ROC AUC': \"{:.3f}\".format(auc),\n",
    "    'F1 Score': \"{:.3f}\".format(f1),\n",
    "    'fpr': fpr,\n",
    "    'tpr': tpr\n",
    "}]\n",
    "\n",
    "df_metrics = pd.DataFrame(results)\n",
    "print(df_metrics[['Label', 'Accuracy', 'Precision', 'Recall', 'ROC AUC', 'F1 Score']])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.plot(fpr, tpr, label=f\"ROC AUC = {auc:.3f}\", color='b')\n",
    "ax.plot([0, 1], [0, 1], 'k--', lw=2, alpha=0.7)\n",
    "ax.legend(loc=\"lower right\")\n",
    "ax.set_xlabel('False Positive Rate')\n",
    "ax.set_ylabel('True Positive Rate')\n",
    "ax.set_title(\"ROC Curve for ResNet Model\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
